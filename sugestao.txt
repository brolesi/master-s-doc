Boa noite, Fábio! Sim, uma sugestão de orientar a sua análise seria essa -- você focar nos dados do Cepagri, e tentar explicar quais modelos preditivos são mais robustos para esses dados que você tem, e por quê.

Para dizer que X é mais eficaz que Y, você precisa argumentar. Uma forma de argumentar é usar a medida de erro, por exemplo. Mas isso é raso demais. Um passo além seria explicar por que X deu uma baixa medida de erro para a série do Cepagri enquanto Y deu alta. Para isso, você deveria ter alguma sacada interessante que justifique essa diferença!

Por exemplo (não estou afirmando que é o caso, é só um exemplo não baseado na realidade!), a série tem sazonalidade, e o modelo X se comporta melhor do que o modelo Y nessa situação porque X é capaz de capturar sazonalidades, enquanto Y nem tento, e portanto X tem uma vantagem sobre Y. (Todas as outras coisas sendo iguais, portanto, X apresenta melhores resultados que Y.) Para afirmar isso, você precisaria mostrar que a série tem sazonalidade: por exemplo (de novo, fictício), as médias dos meses ao longo dos anos tem alta correlação (jan/21 x jan/22 x jan/23 são muito parecidos), o que demonstra a sazonalidade.

Esse tipo de sacada de interpretação dos dados é o que pode ser uma linha da argumentação da conclusão do seu estudo. Por exemplo (de novo, estou sendo criativo aqui, não é o que você propôs!), suponha que sua pergunta seja: "qual a diferença entre o desempenho do modelo X, Y e Z na predição de séries temporais para dados meteorológicos?". Suponha que sua hipótese, a partir da literatura, seja que X e Y tem ótimos desempenhos em séries em geral, mas Z parece ser ainda melhor.

Como você vai investigar? Você vai implementar os modelos X, Y e Z, todos eles usando os mesmos conjuntos de treino e teste. Os modelos vão dar resultado de predição da série, e você vai usar as estatísticas E1, E2 e E3 para mensurar a acurácia desses modelos (E1 é média de erro, E2 é (sei lá!) é o maior erro, etc). E você sabe que, quanto menor E1, E2 e E3, mais acurado o preditor.

Esse é o seu método. Você poderia usar outras medidas também: tempo que demorou o treino de cada modelo, algumas medidas que o modelo tem, como (sei lá!) coeficiente de entropia, estatística de viés dos dados, etc, espaço dimensional dos dados vetoriais de cada modelo (estou inventando coisas!).

Logo vem os resultados: primeiro, você apresenta tudo o que coletou, todos os resultados tabelados. É a parte quantitativa da análise. Depois vem a parte qualitativa: o que esses números significam? X teve E1 e E2 bem mais baixos do que Y e Z. O que isso significa? Aqui vem a explicação: significa que X se adequou melhor ao modelo, por exemplo. E por que o modelo Y teve E2 muito alto? A literatura não falava que X e Y eram equivalentes?

Pois bem, Y teve um valor alto de E1 por causa disso e disso -- overfitting de dados, por exemplo. Você pode checar isso olhando o o espaço dimensional do modelo (inventando, não existe isso!). Por outro lado, Z teve um desempenho melhor que Y, como esperado, mas não melhor que X. Por que? Talvez por causa disso, e disso, e disso.

Daí, para concluir, você retoma a sua pergunta: "qual a diferença entre o desempenho...?". Ora, um se mostrou assim, outro assim, e outro assim. Qual é melhor? Depende! X se mostrou melhor, mas ele demorou muito para treinar, um problema que a literatura já apontava (por exemplo). Z não é tão bom, mas é rápido de treinar e parece ser promissor por causa disso e disso. Enfim, é aqui que você viaja na maionese, divagando um pouco mas sempre falando a partir dos dados, nunca especulando fora das evidências.

O que deve guiar você são os resultados que você obteve. Compare o que você obteve com o que você esperava (você esperava o que a literatura dizia para você esperar), e explique por que seus resultados foram dentro ou fora do esperado. E o que essas informações todas podem lhe ajudar: eu suponho que podem ajudar a construir o melhor modelo possível para predição de dados meteorológicos! Talvez pode ajudar em outras coisas? Pode ajudar a estabelecer métricas de comparações futuras para estudar novos modelos, talvez? Pode ajudar a evidenciar problemas com os tratamentos de dados? Se algum modelo se mostrou ruim, você pode tentar apontar as dificuldades do modelo, e que estratégias usar para melhorar o desempenho do modelo!

Sempre tome cuidado para não generalizar seus resultados sem ter base. Você pode dizer que X sempre é melhor que Y? Se sim, que prova você tem de que outras séries terão o mesmo comportamento? Esse é o melhor dos resultados que você poderia obter, porque afinal, ciência é predição e controle: se você tem essa prova, você basicamente consegue predizer que, se alguém usar o modelo que você analisou em um domínio similar ao que você provou, ele terá os mesmos sucessos!

Enfim, essa é uma sugestão de linha de argumentação! Tem várias, e de vários modos, mas acho que esse é um caminho sadio de pensar o trabalho: questão, hipótese, método para investigar e validar ou refutar a hipótese, experimento, resultados, interpretação, validação ou refutação da hipótese, contribuições que essa informação pode trazer!